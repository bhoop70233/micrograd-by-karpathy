{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RbOP3ZdCo_JF"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# class that mimics the random interface in Python, fully deterministic,\n",
        "# and in a way that we also control fully, and can also use in C, etc.\n",
        "class RNG:\n",
        "    def __init__(self, seed):\n",
        "        self.state = seed\n",
        "\n",
        "    def random_u32(self):\n",
        "        # xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n",
        "        # doing & 0xFFFFFFFFFFFFFFFF is the same as cast to uint64 in C\n",
        "        # doing & 0xFFFFFFFF is the same as cast to uint32 in C\n",
        "        self.state ^= (self.state >> 12) & 0xFFFFFFFFFFFFFFFF\n",
        "        self.state ^= (self.state << 25) & 0xFFFFFFFFFFFFFFFF\n",
        "        self.state ^= (self.state >> 27) & 0xFFFFFFFFFFFFFFFF\n",
        "        return ((self.state * 0x2545F4914F6CDD1D) >> 32) & 0xFFFFFFFF\n",
        "\n",
        "    def random(self):\n",
        "        # random float32 in [0, 1)\n",
        "        return (self.random_u32() >> 8) / 16777216.0\n",
        "\n",
        "    def uniform(self, a=0.0, b=1.0):\n",
        "        # random float32 in [a, b)\n",
        "        return a + (b-a) * self.random()\n",
        "\n",
        "# generate a random dataset with 100 2-dimensional datapoints in 3 classes\n",
        "def gen_data(random: RNG, n=100):\n",
        "    pts = []\n",
        "    for _ in range(n):\n",
        "        x = random.uniform(-2.0, 2.0)\n",
        "        y = random.uniform(-2.0, 2.0)\n",
        "        # concentric circles\n",
        "        # label = 0 if x**2 + y**2 < 1 else 1 if x**2 + y**2 < 2 else 2\n",
        "        # very simple dataset\n",
        "        label = 0 if x < 0 else 1 if y < 0 else 2\n",
        "        pts.append(([x, y], label))\n",
        "    # create train/val/test splits of the data (80%, 10%, 10%)\n",
        "    tr = pts[:int(0.8*n)]\n",
        "    val = pts[int(0.8*n):int(0.9*n)]\n",
        "    te = pts[int(0.9*n):]\n",
        "    return tr, val, te"
      ],
      "metadata": {
        "id": "R6Fa4SOWpiRY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random = RNG(42)\n"
      ],
      "metadata": {
        "id": "q-Ev1743qHeW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Value:\n",
        "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
        "\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "        self.data = data\n",
        "        self.grad = 0\n",
        "        # internal variables used for autograd graph construction\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
        "        out = Value(self.data**other, (self,), f'**{other}')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (other * self.data**(other-1)) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def relu(self):\n",
        "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (out.data > 0) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def tanh(self):\n",
        "        out = Value(math.tanh(self.data), (self,), 'tanh')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (1 - out.data**2) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def exp(self):\n",
        "        out = Value(math.exp(self.data), (self,), 'exp')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += math.exp(self.data) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def log(self):\n",
        "        # (this is the natural log)\n",
        "        out = Value(math.log(self.data), (self,), 'log')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (1/self.data) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "\n",
        "        # topological order all of the children in the graph\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        # go one variable at a time and apply the chain rule to get its gradient\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            v._backward()\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * -1\n",
        "\n",
        "    def __radd__(self, other): # other + self\n",
        "        return self + other\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)\n",
        "\n",
        "    def __rsub__(self, other): # other - self\n",
        "        return other + (-self)\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other): # self / other\n",
        "        return self * other**-1\n",
        "\n",
        "    def __rtruediv__(self, other): # other / self\n",
        "        return other * self**-1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\""
      ],
      "metadata": {
        "id": "N7mP3v0mqPDb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Multi-Layer Perceptron (MLP) network\n",
        "\n",
        "class Module:\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for p in self.parameters():\n",
        "            p.grad = 0\n",
        "\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "class Neuron(Module):\n",
        "\n",
        "    def __init__(self, nin, nonlin=True):\n",
        "        self.w = [Value(random.uniform(-1, 1) * nin**-0.5) for _ in range(nin)]\n",
        "        self.b = Value(0)\n",
        "        self.nonlin = nonlin\n",
        "\n",
        "    def __call__(self, x):\n",
        "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
        "        return act.tanh() if self.nonlin else act\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.w + [self.b]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{'TanH' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
        "\n",
        "class Layer(Module):\n",
        "\n",
        "    def __init__(self, nin, nout, **kwargs):\n",
        "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        out = [n(x) for n in self.neurons]\n",
        "        return out[0] if len(out) == 1 else out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for n in self.neurons for p in n.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
        "\n",
        "class MLP(Module):\n",
        "\n",
        "    def __init__(self, nin, nouts):\n",
        "        sz = [nin] + nouts\n",
        "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for layer in self.layers for p in layer.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
      ],
      "metadata": {
        "id": "r9TcCoBwqZGk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function: the negative log likelihood (NLL) loss\n",
        "# NLL loss = CrossEntropy loss when the targets are one-hot vectors\n",
        "\n",
        "def cross_entropy(logits, target):\n",
        "    # subtract the max for numerical stability (avoids overflow)\n",
        "    max_val = max(val.data for val in logits)\n",
        "    logits = [val - max_val for val in logits]\n",
        "    # 1) evaluate elementwise e^x\n",
        "    ex = [x.exp() for x in logits]\n",
        "    # 2) compute the sum of the above\n",
        "    denom = sum(ex)\n",
        "    # 3) normalize by the sum to get probabilities\n",
        "    probs = [x / denom for x in ex]\n",
        "    # 4) log the probabilities at target\n",
        "    logp = (probs[target]).log()\n",
        "    # 5) the negative log likelihood loss (invert so we get a loss - lower is better)\n",
        "    nll = -logp\n",
        "    return nll\n"
      ],
      "metadata": {
        "id": "XsaD3Be2qd4T"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation utility to compute the loss on a given split of the dataset\n",
        "\n",
        "def eval_split(model, split):\n",
        "    # evaluate the loss of a split\n",
        "    loss = Value(0)\n",
        "    for x, y in split:\n",
        "        logits = model([Value(x[0]), Value(x[1])])\n",
        "        loss += cross_entropy(logits, y)\n",
        "    loss = loss * (1.0/len(split)) # normalize the loss\n",
        "    return loss.data"
      ],
      "metadata": {
        "id": "dAkNCzWfqh7u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# let's train!\n",
        "\n",
        "# generate a random dataset with 100 2-dimensional datapoints in 3 classes\n",
        "train_split, val_split, test_split = gen_data(random, n=100)\n"
      ],
      "metadata": {
        "id": "eJDOWyJPqmRr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init the model: 2D inputs, 16 neurons, 3 outputs (logits)\n",
        "model = MLP(2, [16, 3])\n"
      ],
      "metadata": {
        "id": "quBZwa4Lqrdx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimize using Adam\n",
        "learning_rate = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "weight_decay = 1e-4\n",
        "for p in model.parameters():\n",
        "    p.m = 0.0\n",
        "    p.v = 0.0"
      ],
      "metadata": {
        "id": "TC1P-8GequMt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "for step in range(100):\n",
        "\n",
        "    # evaluate the validation split every few steps\n",
        "    if step % 10 == 0:\n",
        "        val_loss = eval_split(model, val_split)\n",
        "        print(f\"step {step}, val loss {val_loss:.6f}\")\n",
        "\n",
        "    # forward the network (get logits of all training datapoints)\n",
        "    loss = Value(0)\n",
        "    for x, y in train_split:\n",
        "        logits = model([Value(x[0]), Value(x[1])])\n",
        "        loss += cross_entropy(logits, y)\n",
        "    loss = loss * (1.0/len(train_split)) # normalize the loss\n",
        "    # backward pass (deposit the gradients)\n",
        "    loss.backward()\n",
        "    # update with AdamW\n",
        "    for p in model.parameters():\n",
        "        p.m = beta1 * p.m + (1 - beta1) * p.grad\n",
        "        p.v = beta2 * p.v + (1 - beta2) * p.grad**2\n",
        "        m_hat = p.m / (1 - beta1**(step+1))  # bias correction\n",
        "        v_hat = p.v / (1 - beta2**(step+1))\n",
        "        p.data -= learning_rate * (m_hat / (v_hat**0.5 + 1e-8) + weight_decay * p.data)\n",
        "    model.zero_grad() # never forget to clear those gradients! happens to everyone\n",
        "\n",
        "    print(f\"step {step}, train loss {loss.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3-tx-ZQqwua",
        "outputId": "2c6b012c-299a-465a-bdef-cd8ef4b7af67"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, val loss 0.917090\n",
            "step 0, train loss 0.9811897737392364\n",
            "step 1, train loss 0.5148874467883581\n",
            "step 2, train loss 0.3165791889134888\n",
            "step 3, train loss 0.22556573015872267\n",
            "step 4, train loss 0.16796694920743288\n",
            "step 5, train loss 0.1376637835538695\n",
            "step 6, train loss 0.1253639482094802\n",
            "step 7, train loss 0.11410184148424632\n",
            "step 8, train loss 0.1002392391590633\n",
            "step 9, train loss 0.08682478344971886\n",
            "step 10, val loss 0.003921\n",
            "step 10, train loss 0.07694778561832522\n",
            "step 11, train loss 0.07041304536915347\n",
            "step 12, train loss 0.06321229603586982\n",
            "step 13, train loss 0.054156118719272076\n",
            "step 14, train loss 0.04559650177481266\n",
            "step 15, train loss 0.03945831296100577\n",
            "step 16, train loss 0.03563276714098652\n",
            "step 17, train loss 0.032730434581330015\n",
            "step 18, train loss 0.02952248700714751\n",
            "step 19, train loss 0.025962780159194516\n",
            "step 20, val loss 0.002646\n",
            "step 20, train loss 0.022925038178918507\n",
            "step 21, train loss 0.02069198812221886\n",
            "step 22, train loss 0.018933360922337016\n",
            "step 23, train loss 0.017551191329667953\n",
            "step 24, train loss 0.01638179633167417\n",
            "step 25, train loss 0.015224300045331355\n",
            "step 26, train loss 0.014159887463127276\n",
            "step 27, train loss 0.013131954465349264\n",
            "step 28, train loss 0.012120784057507845\n",
            "step 29, train loss 0.01123204107842498\n",
            "step 30, val loss 0.000676\n",
            "step 30, train loss 0.010400258202482048\n",
            "step 31, train loss 0.009651580444066806\n",
            "step 32, train loss 0.009013978996486293\n",
            "step 33, train loss 0.008413772690607866\n",
            "step 34, train loss 0.007889489614215374\n",
            "step 35, train loss 0.007414904403787406\n",
            "step 36, train loss 0.0069558596631992325\n",
            "step 37, train loss 0.006546425763649881\n",
            "step 38, train loss 0.006146836925009589\n",
            "step 39, train loss 0.005766575485785308\n",
            "step 40, val loss 0.000973\n",
            "step 40, train loss 0.005420279697246087\n",
            "step 41, train loss 0.0050819110783730265\n",
            "step 42, train loss 0.004775561542598804\n",
            "step 43, train loss 0.004492905038987439\n",
            "step 44, train loss 0.004224311730420124\n",
            "step 45, train loss 0.003985651760931636\n",
            "step 46, train loss 0.003760277268534241\n",
            "step 47, train loss 0.003551192970327224\n",
            "step 48, train loss 0.0033625767851981807\n",
            "step 49, train loss 0.0031814962875798057\n",
            "step 50, val loss 0.000606\n",
            "step 50, train loss 0.00301575207270546\n",
            "step 51, train loss 0.002862351266993212\n",
            "step 52, train loss 0.002715463062239933\n",
            "step 53, train loss 0.00258159934073884\n",
            "step 54, train loss 0.002454932447291852\n",
            "step 55, train loss 0.0023346984724267514\n",
            "step 56, train loss 0.002224462262598478\n",
            "step 57, train loss 0.0021190534527430844\n",
            "step 58, train loss 0.002020097508889023\n",
            "step 59, train loss 0.0019284533662587526\n",
            "step 60, val loss 0.000376\n",
            "step 60, train loss 0.0018405414107773824\n",
            "step 61, train loss 0.0017585923310738509\n",
            "step 62, train loss 0.0016820578375378393\n",
            "step 63, train loss 0.0016089039286915241\n",
            "step 64, train loss 0.0015409465525300212\n",
            "step 65, train loss 0.0014769675449469132\n",
            "step 66, train loss 0.0014159788335549772\n",
            "step 67, train loss 0.0013591948001180718\n",
            "step 68, train loss 0.0013053152509278979\n",
            "step 69, train loss 0.0012539540551046997\n",
            "step 70, val loss 0.000375\n",
            "step 70, train loss 0.001205807499224382\n",
            "step 71, train loss 0.0011597738212275313\n",
            "step 72, train loss 0.0011158329791148914\n",
            "step 73, train loss 0.0010743830572727006\n",
            "step 74, train loss 0.0010346148383149915\n",
            "step 75, train loss 0.0009966859280539123\n",
            "step 76, train loss 0.0009608002159612319\n",
            "step 77, train loss 0.0009263741227665276\n",
            "step 78, train loss 0.0008935917351144623\n",
            "step 79, train loss 0.0008625091162248249\n",
            "step 80, val loss 0.000355\n",
            "step 80, train loss 0.0008326887882790891\n",
            "step 81, train loss 0.000804277377963405\n",
            "step 82, train loss 0.0007772371817047871\n",
            "step 83, train loss 0.0007512408059560275\n",
            "step 84, train loss 0.0007264004475472227\n",
            "step 85, train loss 0.000702647823818276\n",
            "step 86, train loss 0.000679754264220242\n",
            "step 87, train loss 0.000657811914321428\n",
            "step 88, train loss 0.0006367522431814737\n",
            "step 89, train loss 0.0006164243808268287\n",
            "step 90, val loss 0.000280\n",
            "step 90, train loss 0.000596907114334427\n",
            "step 91, train loss 0.0005781425819395147\n",
            "step 92, train loss 0.0005600318577964423\n",
            "step 93, train loss 0.0005426344354646169\n",
            "step 94, train loss 0.000525896515219057\n",
            "step 95, train loss 0.0005097469488396045\n",
            "step 96, train loss 0.0004942228043717797\n",
            "step 97, train loss 0.00047927058283075177\n",
            "step 98, train loss 0.0004648336049453928\n",
            "step 99, train loss 0.00045093045604465126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z29-47TYq1A6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}