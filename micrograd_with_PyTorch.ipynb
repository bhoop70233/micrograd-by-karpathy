{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Same as micrograd.py, but uses PyTorch for the autograd engine.\n",
        "This is a way for us to check and verify correctness, and also\n",
        "shows some of the similarities/differences in how PyTorch would\n",
        "implement the same MLP. PyTorch lets you specify the forward pass,\n",
        "records all the operations performed, and then calls backward()\n",
        "\"under the hood\" inside its autograd engine.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "tuNUsVANrdNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.parameter import Parameter"
      ],
      "metadata": {
        "id": "mi5oXL5kraxU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# class that mimics the random interface in Python, fully deterministic,\n",
        "# and in a way that we also control fully, and can also use in C, etc.\n",
        "class RNG:\n",
        "    def __init__(self, seed):\n",
        "        self.state = seed\n",
        "\n",
        "    def random_u32(self):\n",
        "        # xorshift rng: https://en.wikipedia.org/wiki/Xorshift#xorshift.2A\n",
        "        # doing & 0xFFFFFFFFFFFFFFFF is the same as cast to uint64 in C\n",
        "        # doing & 0xFFFFFFFF is the same as cast to uint32 in C\n",
        "        self.state ^= (self.state >> 12) & 0xFFFFFFFFFFFFFFFF\n",
        "        self.state ^= (self.state << 25) & 0xFFFFFFFFFFFFFFFF\n",
        "        self.state ^= (self.state >> 27) & 0xFFFFFFFFFFFFFFFF\n",
        "        return ((self.state * 0x2545F4914F6CDD1D) >> 32) & 0xFFFFFFFF\n",
        "\n",
        "    def random(self):\n",
        "        # random float32 in [0, 1)\n",
        "        return (self.random_u32() >> 8) / 16777216.0\n",
        "\n",
        "    def uniform(self, a=0.0, b=1.0):\n",
        "        # random float32 in [a, b)\n",
        "        return a + (b-a) * self.random()\n",
        "\n",
        "# generate a random dataset with 100 2-dimensional datapoints in 3 classes\n",
        "def gen_data(random: RNG, n=100):\n",
        "    pts = []\n",
        "    for _ in range(n):\n",
        "        x = random.uniform(-2.0, 2.0)\n",
        "        y = random.uniform(-2.0, 2.0)\n",
        "        # concentric circles\n",
        "        # label = 0 if x**2 + y**2 < 1 else 1 if x**2 + y**2 < 2 else 2\n",
        "        # very simple dataset\n",
        "        label = 0 if x < 0 else 1 if y < 0 else 2\n",
        "        pts.append(([x, y], label))\n",
        "    # create train/val/test splits of the data (80%, 10%, 10%)\n",
        "    tr = pts[:int(0.8*n)]\n",
        "    val = pts[int(0.8*n):int(0.9*n)]\n",
        "    te = pts[int(0.9*n):]\n",
        "    return tr, val, te"
      ],
      "metadata": {
        "id": "DjTENx5UrinE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random = RNG(42)\n"
      ],
      "metadata": {
        "id": "U4GXgy2rrpVJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Layer Perceptron (MLP) network\n",
        "\n",
        "class Neuron(nn.Module):\n",
        "\n",
        "    def __init__(self, nin, nonlin=True):\n",
        "        super().__init__()\n",
        "        self.w = Parameter(torch.tensor([random.uniform(-1, 1) * nin**-0.5 for _ in range(nin)]))\n",
        "        self.b = Parameter(torch.zeros(1))\n",
        "        self.nonlin = nonlin\n",
        "\n",
        "    def forward(self, x):\n",
        "        act = torch.sum(self.w * x) + self.b\n",
        "        return act.tanh() if self.nonlin else act\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{'TanH' if self.nonlin else 'Linear'}Neuron({len(self.w)})\""
      ],
      "metadata": {
        "id": "Ru4cmjXPrrVp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, nin, nout, **kwargs):\n",
        "        super().__init__()\n",
        "        self.neurons = nn.ModuleList([Neuron(nin, **kwargs) for _ in range(nout)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = [n(x) for n in self.neurons]\n",
        "        return torch.stack(out, dim=-1)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\""
      ],
      "metadata": {
        "id": "Wv8Xo6sJrwpZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, nin, nouts):\n",
        "        super().__init__()\n",
        "        sz = [nin] + nouts\n",
        "        self.layers = nn.ModuleList([Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\""
      ],
      "metadata": {
        "id": "8cCi7N74r21-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's train!\n",
        "\n",
        "train_split, val_split, test_split = gen_data(random, n=100)"
      ],
      "metadata": {
        "id": "NVWfPMWhr9Nv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init the model: 2D inputs, 16 neurons, 3 outputs (logits)\n",
        "model = MLP(2, [16, 3])\n",
        "model.to(torch.float64) # ensure we're using double precision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1R4dlnXr_5M",
        "outputId": "b20e0f76-ca53-4d69-a0b2-107c2701aeca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP of [Layer of [TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2), TanHNeuron(2)], Layer of [LinearNeuron(16), LinearNeuron(16), LinearNeuron(16)]]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def eval_split(model, split):\n",
        "    model.eval()\n",
        "    # evaluate the loss of a split\n",
        "    loss = 0.0\n",
        "    for x, y in split:\n",
        "        logits = model(torch.tensor(x))\n",
        "        y = torch.tensor(y).view(-1)\n",
        "        loss += F.cross_entropy(logits, y).item()\n",
        "    loss = loss * (1.0/len(split)) # normalize the loss\n",
        "    return loss"
      ],
      "metadata": {
        "id": "pj8d3E4csCyH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimize using Adam\n",
        "learning_rate = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "weight_decay = 1e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr=learning_rate,\n",
        "                              betas=(beta1, beta2),\n",
        "                              weight_decay=weight_decay)\n"
      ],
      "metadata": {
        "id": "zIZp8C3IsFzY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "for step in range(100):\n",
        "\n",
        "    # evaluate the validation split every few steps\n",
        "    if step % 10 == 0:\n",
        "        val_loss = eval_split(model, val_split)\n",
        "        print(f\"step {step}, val loss {val_loss}\")\n",
        "\n",
        "    # forward the network (get logits of all training datapoints)\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for x, y in train_split:\n",
        "        logits = model(torch.tensor(x))\n",
        "        loss = F.cross_entropy(logits, torch.tensor(y).view(-1))\n",
        "        losses.append(loss)\n",
        "    loss = torch.stack(losses).mean()\n",
        "    # backward pass (deposit the gradients)\n",
        "    loss.backward()\n",
        "    # update with AdamW\n",
        "    optimizer.step()\n",
        "    model.zero_grad()\n",
        "\n",
        "    print(f\"step {step}, train loss {loss.data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNbpKZy0sLZV",
        "outputId": "d2b1f79d-be48-4e38-9468-e21f7c31c826"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, val loss 0.9170899790006554\n",
            "step 0, train loss 0.9811897723592257\n",
            "step 1, train loss 0.5148874460244011\n",
            "step 2, train loss 0.3165791872557939\n",
            "step 3, train loss 0.22556572887242873\n",
            "step 4, train loss 0.16796694850720875\n",
            "step 5, train loss 0.13766378380671834\n",
            "step 6, train loss 0.12536394889857927\n",
            "step 7, train loss 0.11410184236874368\n",
            "step 8, train loss 0.10023924035832203\n",
            "step 9, train loss 0.08682478525877427\n",
            "step 10, val loss 0.0039209179136893545\n",
            "step 10, train loss 0.07694778828258682\n",
            "step 11, train loss 0.07041304847928265\n",
            "step 12, train loss 0.06321229877098362\n",
            "step 13, train loss 0.05415612070051541\n",
            "step 14, train loss 0.045596503151213816\n",
            "step 15, train loss 0.03945831399740126\n",
            "step 16, train loss 0.03563276787500751\n",
            "step 17, train loss 0.03273043483103781\n",
            "step 18, train loss 0.029522486797248632\n",
            "step 19, train loss 0.025962779805612846\n",
            "step 20, val loss 0.002645744970112932\n",
            "step 20, train loss 0.022925037924814106\n",
            "step 21, train loss 0.0206919879692237\n",
            "step 22, train loss 0.018933360794902338\n",
            "step 23, train loss 0.017551191204801052\n",
            "step 24, train loss 0.016381796204712872\n",
            "step 25, train loss 0.015224299928052187\n",
            "step 26, train loss 0.014159887397100963\n",
            "step 27, train loss 0.013131954462967605\n",
            "step 28, train loss 0.012120784111950084\n",
            "step 29, train loss 0.011232041183027962\n",
            "step 30, val loss 0.000676040362513603\n",
            "step 30, train loss 0.010400258330764777\n",
            "step 31, train loss 0.009651580577762863\n",
            "step 32, train loss 0.009013979127643835\n",
            "step 33, train loss 0.00841377280606163\n",
            "step 34, train loss 0.00788948971401162\n",
            "step 35, train loss 0.007414904491305866\n",
            "step 36, train loss 0.006955859737478374\n",
            "step 37, train loss 0.006546425832534136\n",
            "step 38, train loss 0.006146836991564805\n",
            "step 39, train loss 0.005766575549549978\n",
            "step 40, val loss 0.0009726681462488927\n",
            "step 40, train loss 0.005420279762596479\n",
            "step 41, train loss 0.005081911142465393\n",
            "step 42, train loss 0.004775561604020663\n",
            "step 43, train loss 0.004492905099500602\n",
            "step 44, train loss 0.004224311786232263\n",
            "step 45, train loss 0.003985651812982528\n",
            "step 46, train loss 0.0037602773175859526\n",
            "step 47, train loss 0.0035511930138428204\n",
            "step 48, train loss 0.003362576825665979\n",
            "step 49, train loss 0.003181496324510864\n",
            "step 50, val loss 0.0006058722752017593\n",
            "step 50, train loss 0.0030157521050632255\n",
            "step 51, train loss 0.0028623512972954413\n",
            "step 52, train loss 0.0027154630892614585\n",
            "step 53, train loss 0.002581599364876447\n",
            "step 54, train loss 0.0024549324703658933\n",
            "step 55, train loss 0.002334698493058433\n",
            "step 56, train loss 0.0022244622819131365\n",
            "step 57, train loss 0.0021190534716016445\n",
            "step 58, train loss 0.0020200975261518875\n",
            "step 59, train loss 0.0019284533832589295\n",
            "step 60, val loss 0.0003763502700309553\n",
            "step 60, train loss 0.0018405414275557224\n",
            "step 61, train loss 0.0017585923468297336\n",
            "step 62, train loss 0.0016820578534257028\n",
            "step 63, train loss 0.0016089039442681815\n",
            "step 64, train loss 0.0015409465673814741\n",
            "step 65, train loss 0.0014769675599397285\n",
            "step 66, train loss 0.001415978848036822\n",
            "step 67, train loss 0.0013591948139511042\n",
            "step 68, train loss 0.0013053152646999877\n",
            "step 69, train loss 0.0012539540681779826\n",
            "step 70, val loss 0.00037463483322978957\n",
            "step 70, train loss 0.0012058075116795034\n",
            "step 71, train loss 0.0011597738334617191\n",
            "step 72, train loss 0.0011158329906160815\n",
            "step 73, train loss 0.0010743830682596552\n",
            "step 74, train loss 0.001034614849060202\n",
            "step 75, train loss 0.0009966859381780266\n",
            "step 76, train loss 0.0009608002257246454\n",
            "step 77, train loss 0.0009263741323226333\n",
            "step 78, train loss 0.0008935917441641759\n",
            "step 79, train loss 0.0008625091249952803\n",
            "step 80, val loss 0.00035477097901794106\n",
            "step 80, train loss 0.0008326887968219914\n",
            "step 81, train loss 0.0008042773860365147\n",
            "step 82, train loss 0.0007772371894912517\n",
            "step 83, train loss 0.0007512408134498054\n",
            "step 84, train loss 0.0007264004545655242\n",
            "step 85, train loss 0.0007026478305174581\n",
            "step 86, train loss 0.000679754270581201\n",
            "step 87, train loss 0.0006578119202284664\n",
            "step 88, train loss 0.0006367522487798384\n",
            "step 89, train loss 0.0006164243861023226\n",
            "step 90, val loss 0.00028012676028549847\n",
            "step 90, train loss 0.000596907119221627\n",
            "step 91, train loss 0.0005781425865665143\n",
            "step 92, train loss 0.0005600318621503035\n",
            "step 93, train loss 0.0005426344395068431\n",
            "step 94, train loss 0.000525896519049427\n",
            "step 95, train loss 0.0005097469524366745\n",
            "step 96, train loss 0.0004942228077062102\n",
            "step 97, train loss 0.0004792705859744544\n",
            "step 98, train loss 0.00046483360787282125\n",
            "step 99, train loss 0.0004509304587360192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eTSnEzkisPTI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}